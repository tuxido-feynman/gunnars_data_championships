{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"data\"]).decode(\"utf8\"))\n",
    "from sklearn import metrics, model_selection\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4211.039202</td>\n",
       "      <td>0.019007</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.074364</td>\n",
       "      <td>0.061060</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.010216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325968</td>\n",
       "      <td>0.049656</td>\n",
       "      <td>0.311951</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.001663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2423.078926</td>\n",
       "      <td>0.136565</td>\n",
       "      <td>0.015414</td>\n",
       "      <td>0.262394</td>\n",
       "      <td>0.239468</td>\n",
       "      <td>0.494832</td>\n",
       "      <td>0.026691</td>\n",
       "      <td>0.051061</td>\n",
       "      <td>0.093357</td>\n",
       "      <td>0.100570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468791</td>\n",
       "      <td>0.217258</td>\n",
       "      <td>0.463345</td>\n",
       "      <td>0.137399</td>\n",
       "      <td>0.108356</td>\n",
       "      <td>0.089524</td>\n",
       "      <td>0.093357</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.026691</td>\n",
       "      <td>0.040752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2115.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4202.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6310.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8416.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID          X10          X11          X12          X13  \\\n",
       "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \n",
       "mean   4211.039202     0.019007     0.000238     0.074364     0.061060   \n",
       "std    2423.078926     0.136565     0.015414     0.262394     0.239468   \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%    2115.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%    4202.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%    6310.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max    8416.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               X14          X15          X16          X17          X18  \\\n",
       "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \n",
       "mean      0.427893     0.000713     0.002613     0.008791     0.010216   \n",
       "std       0.494832     0.026691     0.051061     0.093357     0.100570   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          ...              X375         X376         X377         X378  \\\n",
       "count     ...       4209.000000  4209.000000  4209.000000  4209.000000   \n",
       "mean      ...          0.325968     0.049656     0.311951     0.019244   \n",
       "std       ...          0.468791     0.217258     0.463345     0.137399   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          1.000000     0.000000     1.000000     0.000000   \n",
       "max       ...          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              X379         X380         X382         X383         X384  \\\n",
       "count  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \n",
       "mean      0.011879     0.008078     0.008791     0.000475     0.000713   \n",
       "std       0.108356     0.089524     0.093357     0.021796     0.026691   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              X385  \n",
       "count  4209.000000  \n",
       "mean      0.001663  \n",
       "std       0.040752  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 369 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "train.describe()\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.loc[train['y']!=train['y'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(['ID', 'y'], axis=1)\n",
    "y = train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) X314                           0.393906\n",
      " 2) X315                           0.075398\n",
      " 3) X5                             0.065658\n",
      " 4) X6                             0.034147\n",
      " 5) X8                             0.031934\n",
      " 6) X118                           0.028506\n",
      " 7) X119                           0.027816\n",
      " 8) X263                           0.018507\n",
      " 9) X3                             0.016027\n",
      "10) X136                           0.013498\n",
      "11) X0                             0.012337\n",
      "12) X1                             0.010123\n",
      "13) X127                           0.008401\n",
      "14) X29                            0.007948\n",
      "15) X232                           0.007873\n",
      "16) X279                           0.007530\n",
      "17) X2                             0.007009\n",
      "18) X54                            0.005940\n",
      "19) X47                            0.005237\n",
      "20) X189                           0.005119\n",
      "21) X76                            0.004751\n",
      "22) X46                            0.003595\n",
      "23) X27                            0.003410\n",
      "24) X151                           0.003167\n",
      "25) X51                            0.003119\n",
      "26) X342                           0.003076\n",
      "27) X345                           0.003008\n",
      "28) X351                           0.002928\n",
      "29) X218                           0.002890\n",
      "30) X354                           0.002734\n",
      "31) X64                            0.002721\n",
      "32) X339                           0.002579\n",
      "33) X225                           0.002413\n",
      "34) X220                           0.002406\n",
      "35) X273                           0.002406\n",
      "36) X283                           0.002309\n",
      "37) X12                            0.002268\n",
      "38) X163                           0.002250\n",
      "39) X201                           0.002177\n",
      "40) X142                           0.002174\n",
      "41) X81                            0.002116\n",
      "42) X104                           0.002108\n",
      "43) X158                           0.002036\n",
      "44) X383                           0.002029\n",
      "45) X50                            0.001950\n",
      "46) X116                           0.001939\n",
      "47) X100                           0.001923\n",
      "48) X350                           0.001914\n",
      "49) X375                           0.001898\n",
      "50) X327                           0.001842\n",
      "51) X340                           0.001798\n",
      "52) X139                           0.001738\n",
      "53) X355                           0.001734\n",
      "54) X267                           0.001712\n",
      "55) X322                           0.001681\n",
      "56) X70                            0.001659\n",
      "57) X181                           0.001655\n",
      "58) X85                            0.001629\n",
      "59) X73                            0.001612\n",
      "60) X137                           0.001611\n",
      "61) X275                           0.001569\n",
      "62) X164                           0.001554\n",
      "63) X109                           0.001546\n",
      "64) X150                           0.001539\n",
      "65) X168                           0.001519\n",
      "66) X58                            0.001507\n",
      "67) X13                            0.001502\n",
      "68) X115                           0.001500\n",
      "69) X287                           0.001498\n",
      "70) X177                           0.001496\n",
      "71) X154                           0.001495\n",
      "72) X324                           0.001475\n",
      "73) X144                           0.001470\n",
      "74) X77                            0.001462\n",
      "75) X171                           0.001442\n",
      "76) X156                           0.001434\n",
      "77) X341                           0.001430\n",
      "78) X20                            0.001418\n",
      "79) X374                           0.001393\n",
      "80) X224                           0.001389\n",
      "81) X68                            0.001386\n",
      "82) X223                           0.001354\n",
      "83) X300                           0.001330\n",
      "84) X133                           0.001320\n",
      "85) X157                           0.001308\n",
      "86) X132                           0.001297\n",
      "87) X22                            0.001294\n",
      "88) X241                           0.001277\n",
      "89) X321                           0.001276\n",
      "90) X336                           0.001232\n",
      "91) X205                           0.001229\n",
      "92) X35                            0.001226\n",
      "93) X329                           0.001221\n",
      "94) X31                            0.001207\n",
      "95) X311                           0.001171\n",
      "96) X180                           0.001165\n",
      "97) X65                            0.001145\n",
      "98) X294                           0.001125\n",
      "99) X23                            0.001113\n",
      "100) X37                            0.001106\n",
      "101) X96                            0.001071\n",
      "102) X103                           0.001058\n",
      "103) X363                           0.001050\n",
      "104) X95                            0.001043\n",
      "105) X301                           0.001043\n",
      "106) X152                           0.001033\n",
      "107) X226                           0.001030\n",
      "108) X182                           0.001022\n",
      "109) X326                           0.000977\n",
      "110) X14                            0.000945\n",
      "111) X316                           0.000934\n",
      "112) X244                           0.000932\n",
      "113) X313                           0.000932\n",
      "114) X131                           0.000931\n",
      "115) X191                           0.000930\n",
      "116) X291                           0.000927\n",
      "117) X337                           0.000917\n",
      "118) X358                           0.000901\n",
      "119) X176                           0.000883\n",
      "120) X285                           0.000873\n",
      "121) X114                           0.000853\n",
      "122) X26                            0.000851\n",
      "123) X84                            0.000840\n",
      "124) X71                            0.000825\n",
      "125) X286                           0.000810\n",
      "126) X377                           0.000790\n",
      "127) X334                           0.000768\n",
      "128) X209                           0.000745\n",
      "129) X376                           0.000713\n",
      "130) X246                           0.000706\n",
      "131) X173                           0.000706\n",
      "132) X141                           0.000693\n",
      "133) X195                           0.000690\n",
      "134) X86                            0.000684\n",
      "135) X219                           0.000681\n",
      "136) X43                            0.000673\n",
      "137) X129                           0.000647\n",
      "138) X343                           0.000643\n",
      "139) X155                           0.000640\n",
      "140) X360                           0.000640\n",
      "141) X362                           0.000634\n",
      "142) X250                           0.000608\n",
      "143) X4                             0.000602\n",
      "144) X49                            0.000600\n",
      "145) X45                            0.000593\n",
      "146) X284                           0.000592\n",
      "147) X178                           0.000591\n",
      "148) X359                           0.000586\n",
      "149) X38                            0.000585\n",
      "150) X356                           0.000584\n",
      "151) X41                            0.000583\n",
      "152) X82                            0.000582\n",
      "153) X69                            0.000555\n",
      "154) X175                           0.000551\n",
      "155) X117                           0.000539\n",
      "156) X196                           0.000537\n",
      "157) X79                            0.000528\n",
      "158) X229                           0.000519\n",
      "159) X231                           0.000515\n",
      "160) X251                           0.000489\n",
      "161) X186                           0.000484\n",
      "162) X234                           0.000483\n",
      "163) X349                           0.000477\n",
      "164) X161                           0.000474\n",
      "165) X187                           0.000468\n",
      "166) X105                           0.000457\n",
      "167) X194                           0.000452\n",
      "168) X361                           0.000439\n",
      "169) X215                           0.000427\n",
      "170) X19                            0.000419\n",
      "171) X368                           0.000410\n",
      "172) X143                           0.000393\n",
      "173) X333                           0.000376\n",
      "174) X206                           0.000372\n",
      "175) X373                           0.000368\n",
      "176) X61                            0.000367\n",
      "177) X247                           0.000361\n",
      "178) X145                           0.000361\n",
      "179) X365                           0.000360\n",
      "180) X138                           0.000358\n",
      "181) X240                           0.000356\n",
      "182) X228                           0.000356\n",
      "183) X256                           0.000354\n",
      "184) X56                            0.000353\n",
      "185) X135                           0.000353\n",
      "186) X140                           0.000342\n",
      "187) X364                           0.000338\n",
      "188) X146                           0.000336\n",
      "189) X208                           0.000330\n",
      "190) X52                            0.000328\n",
      "191) X202                           0.000310\n",
      "192) X120                           0.000305\n",
      "193) X34                            0.000291\n",
      "194) X344                           0.000286\n",
      "195) X174                           0.000280\n",
      "196) X353                           0.000279\n",
      "197) X165                           0.000272\n",
      "198) X338                           0.000251\n",
      "199) X292                           0.000248\n",
      "200) X147                           0.000247\n",
      "201) X48                            0.000246\n",
      "202) X222                           0.000238\n",
      "203) X332                           0.000237\n",
      "204) X18                            0.000233\n",
      "205) X128                           0.000227\n",
      "206) X10                            0.000225\n",
      "207) X306                           0.000218\n",
      "208) X113                           0.000216\n",
      "209) X148                           0.000215\n",
      "210) X134                           0.000205\n",
      "211) X204                           0.000197\n",
      "212) X379                           0.000194\n",
      "213) X63                            0.000191\n",
      "214) X346                           0.000191\n",
      "215) X304                           0.000188\n",
      "216) X170                           0.000186\n",
      "217) X169                           0.000184\n",
      "218) X271                           0.000182\n",
      "219) X197                           0.000180\n",
      "220) X203                           0.000177\n",
      "221) X83                            0.000175\n",
      "222) X98                            0.000169\n",
      "223) X111                           0.000169\n",
      "224) X380                           0.000167\n",
      "225) X367                           0.000167\n",
      "226) X75                            0.000166\n",
      "227) X162                           0.000159\n",
      "228) X305                           0.000158\n",
      "229) X211                           0.000154\n",
      "230) X16                            0.000145\n",
      "231) X179                           0.000140\n",
      "232) X331                           0.000140\n",
      "233) X101                           0.000134\n",
      "234) X352                           0.000129\n",
      "235) X298                           0.000121\n",
      "236) X366                           0.000120\n",
      "237) X28                            0.000118\n",
      "238) X238                           0.000117\n",
      "239) X97                            0.000114\n",
      "240) X236                           0.000110\n",
      "241) X78                            0.000105\n",
      "242) X299                           0.000105\n",
      "243) X312                           0.000100\n",
      "244) X130                           0.000099\n",
      "245) X254                           0.000095\n",
      "246) X309                           0.000094\n",
      "247) X230                           0.000089\n",
      "248) X106                           0.000078\n",
      "249) X36                            0.000077\n",
      "250) X323                           0.000073\n",
      "251) X328                           0.000073\n",
      "252) X123                           0.000070\n",
      "253) X91                            0.000070\n",
      "254) X200                           0.000070\n",
      "255) X153                           0.000064\n",
      "256) X378                           0.000058\n",
      "257) X371                           0.000056\n",
      "258) X108                           0.000056\n",
      "259) X185                           0.000055\n",
      "260) X212                           0.000048\n",
      "261) X265                           0.000044\n",
      "262) X348                           0.000044\n",
      "263) X55                            0.000041\n",
      "264) X66                            0.000041\n",
      "265) X80                            0.000041\n",
      "266) X32                            0.000040\n",
      "267) X57                            0.000035\n",
      "268) X166                           0.000030\n",
      "269) X282                           0.000027\n",
      "270) X184                           0.000027\n",
      "271) X243                           0.000025\n",
      "272) X126                           0.000025\n",
      "273) X217                           0.000025\n",
      "274) X237                           0.000023\n",
      "275) X255                           0.000022\n",
      "276) X213                           0.000022\n",
      "277) X24                            0.000022\n",
      "278) X30                            0.000020\n",
      "279) X262                           0.000020\n",
      "280) X266                           0.000020\n",
      "281) X221                           0.000019\n",
      "282) X249                           0.000019\n",
      "283) X198                           0.000019\n",
      "284) X67                            0.000019\n",
      "285) X122                           0.000019\n",
      "286) X90                            0.000018\n",
      "287) X99                            0.000018\n",
      "288) X40                            0.000018\n",
      "289) X21                            0.000018\n",
      "290) X74                            0.000018\n",
      "291) X320                           0.000017\n",
      "292) X242                           0.000017\n",
      "293) X274                           0.000016\n",
      "294) X264                           0.000016\n",
      "295) X192                           0.000016\n",
      "296) X110                           0.000015\n",
      "297) X302                           0.000015\n",
      "298) X272                           0.000015\n",
      "299) X227                           0.000014\n",
      "300) X260                           0.000014\n",
      "301) X88                            0.000012\n",
      "302) X125                           0.000012\n",
      "303) X94                            0.000011\n",
      "304) X276                           0.000011\n",
      "305) X382                           0.000011\n",
      "306) X44                            0.000011\n",
      "307) X307                           0.000010\n",
      "308) X317                           0.000010\n",
      "309) X159                           0.000010\n",
      "310) X319                           0.000010\n",
      "311) X325                           0.000009\n",
      "312) X17                            0.000009\n",
      "313) X89                            0.000009\n",
      "314) X15                            0.000009\n",
      "315) X261                           0.000009\n",
      "316) X357                           0.000008\n",
      "317) X183                           0.000007\n",
      "318) X308                           0.000007\n",
      "319) X369                           0.000007\n",
      "320) X370                           0.000007\n",
      "321) X190                           0.000006\n",
      "322) X245                           0.000006\n",
      "323) X216                           0.000006\n",
      "324) X288                           0.000006\n",
      "325) X310                           0.000006\n",
      "326) X253                           0.000006\n",
      "327) X87                            0.000005\n",
      "328) X112                           0.000005\n",
      "329) X372                           0.000004\n",
      "330) X53                            0.000004\n",
      "331) X199                           0.000004\n",
      "332) X172                           0.000004\n",
      "333) X385                           0.000004\n",
      "334) X295                           0.000004\n",
      "335) X281                           0.000003\n",
      "336) X60                            0.000003\n",
      "337) X239                           0.000003\n",
      "338) X214                           0.000003\n",
      "339) X384                           0.000003\n",
      "340) X318                           0.000003\n",
      "341) X102                           0.000003\n",
      "342) X258                           0.000003\n",
      "343) X252                           0.000002\n",
      "344) X160                           0.000002\n",
      "345) X62                            0.000002\n",
      "346) X248                           0.000002\n",
      "347) X39                            0.000002\n",
      "348) X277                           0.000002\n",
      "349) X124                           0.000001\n",
      "350) X207                           0.000001\n",
      "351) X296                           0.000001\n",
      "352) X92                            0.000001\n",
      "353) X42                            0.000001\n",
      "354) X280                           0.000001\n",
      "355) X33                            0.000001\n",
      "356) X335                           0.000000\n",
      "357) X278                           0.000000\n",
      "358) X269                           0.000000\n",
      "359) X167                           0.000000\n",
      "360) X270                           0.000000\n",
      "361) X257                           0.000000\n",
      "362) X59                            0.000000\n",
      "363) X259                           0.000000\n",
      "364) X210                           0.000000\n",
      "365) X290                           0.000000\n",
      "366) X93                            0.000000\n",
      "367) X330                           0.000000\n",
      "368) X297                           0.000000\n",
      "369) X293                           0.000000\n",
      "370) X347                           0.000000\n",
      "371) X289                           0.000000\n",
      "372) X233                           0.000000\n",
      "373) X11                            0.000000\n",
      "374) X268                           0.000000\n",
      "375) X235                           0.000000\n",
      "376) X107                           0.000000\n"
     ]
    }
   ],
   "source": [
    "feature_labels = X.columns\n",
    "forest = RandomForestRegressor(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "forest.fit(X, y)    # Fits the Random Forest Regressor to the entire data set.\n",
    "importances = forest.feature_importances_  # Sets importances equal to the feature importances of the model\n",
    "indices = np.argsort(importances)[::-1]\n",
    "order_features = []\n",
    "order_importances = []\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f+1, 30, feature_labels[indices[f]], importances[indices[f]]))\n",
    "    order_features.append(feature_labels[f])\n",
    "    order_importances.append(importances[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJUCAYAAADXbWgUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x101dcd6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "top_50_importances = order_importances[:50] #This will give us the top 50 features by importance\n",
    "plt.title('Top 50 Features By Importance')\n",
    "plt.bar(range(X.shape[1]-326), top_50_importances, color='lightblue', align='center')\n",
    "plt.xticks(range(X.shape[1]-326), order_features[:50], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]-326])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.drop(order_features[250:], axis=1)\n",
    "train = train.drop(order_features[250:], axis=1) # Modify train to only take in the top 100 features and the target column y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 251 entries, ID to X257\n",
      "dtypes: int64(251)\n",
      "memory usage: 8.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         ID       y  X0  X1  X2  X3  X4  X5  X6  X8  ...   X248  X249  X250  \\\n",
       "0        0  130.81  37  23  20   0   3  27   9  14  ...      0     0     0   \n",
       "1        6   88.53  37  21  22   4   3  31  11  14  ...      0     0     1   \n",
       "2        7   76.26  24  24  38   2   3  30   9  23  ...      0     0     1   \n",
       "3        9   80.62  24  21  38   5   3  30  11   4  ...      0     0     1   \n",
       "4       13   78.02  24  23  38   5   3  14   3  13  ...      0     0     1   \n",
       "5       18   92.93  46   3  29   2   3  13   7  18  ...      0     0     1   \n",
       "6       24  128.76  11  19  29   5   3  12   7  18  ...      0     0     1   \n",
       "7       25   91.91  41  13  19   5   3  12   9   0  ...      0     0     0   \n",
       "8       27  108.67  49  20  19   4   3  12   8   7  ...      0     0     0   \n",
       "9       30  126.99  36   3  17   2   3  12   0   4  ...      0     0     1   \n",
       "10      31  102.09  34  19  42   5   3  12   7  15  ...      0     0     1   \n",
       "11      32   98.12  11  19  29   5   3  12   7  14  ...      0     0     1   \n",
       "12      34   82.62  45   3   9   2   3  12   6  12  ...      0     0     1   \n",
       "13      36   94.12  11  19  29   5   3  16   7  14  ...      0     0     1   \n",
       "14      37   99.15  41  20  19   4   3  16   6  12  ...      0     0     0   \n",
       "15      38   93.64  40  13  11   5   3  16   3  10  ...      0     0     1   \n",
       "16      39  106.10  11  19  29   5   3  16   7  14  ...      0     0     1   \n",
       "17      40  114.13  23   1  19   2   3  16   2   3  ...      0     0     0   \n",
       "18      44   89.81  32   4  37   2   3  16   3   4  ...      0     0     1   \n",
       "19      47   90.81  40   0  11   5   3  16   8  18  ...      0     0     1   \n",
       "20      48   90.56  32  20  37   2   3  16   9   8  ...      0     0     1   \n",
       "21      49   94.57  46   1  19   3   3  16   9  18  ...      0     0     0   \n",
       "22      50  108.14  50  19   0   1   3  16   9  14  ...      0     0     1   \n",
       "23      52  120.77  50   3  37   2   3  16   9  13  ...      0     0     1   \n",
       "24      54   84.84  51  19  11   3   3  16   3  18  ...      0     0     1   \n",
       "25      60   93.59  41  20  37   2   3  16   8  21  ...      0     0     1   \n",
       "26      61  104.07   9  13  19   5   3  16   6   9  ...      0     0     0   \n",
       "27      62   89.37  46   1  19   2   3  16   3   1  ...      0     0     0   \n",
       "28      66   90.08  32  20  19   2   3  16   2  16  ...      0     0     0   \n",
       "29      67  128.19  50   3  37   2   3  16   9  13  ...      0     0     1   \n",
       "...    ...     ...  ..  ..  ..  ..  ..  ..  ..  ..  ...    ...   ...   ...   \n",
       "4179  8354   85.93  11   0  11   5   3   1  11  19  ...      0     0     1   \n",
       "4180  8356   90.45  26  23   9   0   3   1  11   0  ...      0     0     1   \n",
       "4181  8357   90.06  51  19  19   5   3   1   6  21  ...      0     0     0   \n",
       "4182  8362   90.38  52  23   5   2   3   1   6  17  ...      0     0     1   \n",
       "4183  8367   95.56  52   1  37   2   3   1  11  18  ...      0     0     1   \n",
       "4184  8368  109.00  49  20  19   2   3   1   3   8  ...      0     0     0   \n",
       "4185  8369  109.64  10  23  15   3   3   1   6  15  ...      0     0     0   \n",
       "4186  8371  131.98  36   7  18   0   3   1   9  13  ...      0     0     1   \n",
       "4187  8373   98.15   9  13   5   5   3   1   9   9  ...      0     0     1   \n",
       "4188  8374  102.33  32  20  19   0   3   1   3  19  ...      0     0     0   \n",
       "4189  8375  102.42   9  20  19   2   3   1   3  16  ...      0     0     0   \n",
       "4190  8378   89.11  40  20   5   2   3   1   9   5  ...      0     0     1   \n",
       "4191  8382   88.93  52  23   5   2   3   1   3  17  ...      0     0     1   \n",
       "4192  8383  103.03   9  13  19   5   3   1   9  13  ...      0     0     0   \n",
       "4193  8384  107.24  10  23  23   2   3   1   3  17  ...      0     0     0   \n",
       "4194  8385   91.13  52  16  19   5   3   1   8   9  ...      0     0     0   \n",
       "4195  8387   86.23  41  13   5   5   3   1   6   9  ...      0     0     1   \n",
       "4196  8390   99.93  51  16  37   5   3   1  11  24  ...      0     0     1   \n",
       "4197  8392   89.25  52  23   5   2   3   1   3  17  ...      0     0     1   \n",
       "4198  8393   97.09  21   0  11   5   3   1   7  14  ...      0     0     1   \n",
       "4199  8395   88.24  46   1  25   2   3   1  11  14  ...      0     0     1   \n",
       "4200  8397  108.59  52   1  29   2   3   1   8  22  ...      0     0     1   \n",
       "4201  8399  107.39  49  23  44   3   3   1   7   6  ...      0     0     0   \n",
       "4202  8402  123.34  15  13  43   2   3   1   3  17  ...      0     0     0   \n",
       "4203  8403   85.71  16  20  19   2   3   1   0   6  ...      0     0     0   \n",
       "4204  8405  107.39  10  20  19   2   3   1   3  16  ...      0     0     0   \n",
       "4205  8406  108.77  36  16  44   3   3   1   7   7  ...      0     0     0   \n",
       "4206  8412  109.22  10  23  42   0   3   1   6   4  ...      0     0     1   \n",
       "4207  8415   87.48  11  19  29   5   3   1  11  20  ...      0     0     1   \n",
       "4208  8417  110.85  52  19   5   2   3   1   6  22  ...      0     0     1   \n",
       "\n",
       "      X251  X252  X253  X254  X255  X256  X257  \n",
       "0        0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0  \n",
       "2        0     1     0     0     0     1     0  \n",
       "3        0     1     0     0     0     1     0  \n",
       "4        0     1     0     0     0     1     0  \n",
       "5        0     0     0     0     0     0     0  \n",
       "6        0     0     0     0     0     0     0  \n",
       "7        1     0     0     0     0     0     0  \n",
       "8        1     0     0     0     0     0     0  \n",
       "9        0     0     0     0     0     0     0  \n",
       "10       0     0     0     0     0     0     0  \n",
       "11       0     0     0     0     0     0     0  \n",
       "12       0     0     0     0     0     0     0  \n",
       "13       0     0     0     0     0     0     0  \n",
       "14       1     0     0     0     0     0     0  \n",
       "15       0     0     0     0     0     0     0  \n",
       "16       0     0     0     0     0     0     0  \n",
       "17       1     0     0     0     0     0     0  \n",
       "18       0     0     0     0     0     0     0  \n",
       "19       0     0     0     0     0     0     0  \n",
       "20       0     0     0     0     0     0     0  \n",
       "21       1     0     0     0     0     0     0  \n",
       "22       0     0     0     0     0     1     0  \n",
       "23       0     0     0     0     0     0     0  \n",
       "24       0     0     0     0     0     0     0  \n",
       "25       0     0     0     0     0     0     0  \n",
       "26       1     0     0     0     0     0     0  \n",
       "27       1     0     0     0     0     0     0  \n",
       "28       1     0     0     0     0     0     0  \n",
       "29       0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...  \n",
       "4179     0     0     0     0     0     0     0  \n",
       "4180     0     0     0     0     0     0     0  \n",
       "4181     1     0     0     0     0     0     0  \n",
       "4182     0     0     0     0     0     0     0  \n",
       "4183     0     0     0     0     0     0     0  \n",
       "4184     1     0     0     0     0     0     0  \n",
       "4185     0     0     0     0     0     0     0  \n",
       "4186     0     0     0     0     0     0     0  \n",
       "4187     0     0     0     0     0     0     0  \n",
       "4188     1     0     0     0     0     0     0  \n",
       "4189     1     0     0     0     0     0     0  \n",
       "4190     0     0     0     0     0     0     0  \n",
       "4191     0     0     0     0     0     0     0  \n",
       "4192     1     0     0     0     0     0     0  \n",
       "4193     0     0     0     0     0     0     0  \n",
       "4194     1     0     0     0     0     0     0  \n",
       "4195     0     0     0     0     0     0     0  \n",
       "4196     0     0     0     0     0     0     0  \n",
       "4197     0     0     0     0     0     0     0  \n",
       "4198     0     0     0     0     0     0     0  \n",
       "4199     0     0     0     0     0     1     0  \n",
       "4200     0     0     0     0     0     0     0  \n",
       "4201     0     0     0     0     1     1     0  \n",
       "4202     0     0     0     0     0     0     0  \n",
       "4203     1     0     0     0     0     0     0  \n",
       "4204     1     0     0     0     0     0     0  \n",
       "4205     0     0     0     0     1     1     0  \n",
       "4206     0     0     0     0     0     0     0  \n",
       "4207     0     0     0     0     0     0     0  \n",
       "4208     0     0     0     0     0     0     0  \n",
       "\n",
       "[4208 rows x 252 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.info()\n",
    "train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 251 entries, ID to X257\n",
      "dtypes: int64(251)\n",
      "memory usage: 8.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amcfarlin/Projects/gunnars_data_championships/venv/lib/python3.4/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "test.info()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train the xgb model then predict the test data'''\n",
    "\n",
    "xgb_params = {\n",
    "    'n_trees': 520, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "num_boost_rounds = 1250\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data:\n",
      "0.663017078306\n",
      "Cross Validation\n",
      "................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amcfarlin/Projects/gunnars_data_championships/venv/lib/python3.4/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 (3346, 311) (862, 311)\n",
      "Fitting XGBoost for Fold 1\n",
      "Fitting stacked pipeline for Fold 1\n",
      "0.566511504938\n",
      "Fold 2 (3360, 311) (848, 311)\n",
      "Fitting XGBoost for Fold 2\n",
      "Fitting stacked pipeline for Fold 2\n",
      "0.592925612287\n",
      "Fold 3 (3383, 311) (825, 311)\n",
      "Fitting XGBoost for Fold 3\n",
      "Fitting stacked pipeline for Fold 3\n",
      "0.559456020202\n",
      "Fold 4 (3390, 311) (818, 311)\n",
      "Fitting XGBoost for Fold 4\n",
      "Fitting stacked pipeline for Fold 4\n",
      "0.602125710581\n",
      "Fold 5 (3353, 311) (855, 311)\n",
      "Fitting XGBoost for Fold 5\n",
      "Fitting stacked pipeline for Fold 5\n",
      "0.601508910956\n"
     ]
    }
   ],
   "source": [
    "'''Train the stacked models then predict the test data'''\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=ElasticNetCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=5, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7, n_estimators=200)),\n",
    "    ElasticNetCV()\n",
    "\n",
    ")\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "'''R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('R2 score on train data:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "\n",
    "\n",
    "\n",
    "'''Average the predictions test data of both models then save it on a csv file'''\n",
    "\n",
    "print('Cross Validation')\n",
    "print('................')\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.StratifiedKFold(n_splits=n_folds, random_state=1, shuffle=True)\n",
    "\n",
    "X = train.drop('y', axis=1).values\n",
    "y = train['y'].values\n",
    "\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    fold += 1\n",
    "    \n",
    "    X_training, X_valid = X[train_index], X[test_index]\n",
    "    y_training, y_valid = y[train_index], y[test_index]\n",
    "    \n",
    "    finaltrainset = train[usable_columns].values\n",
    "    final_train, final_valid = finaltrainset[train_index], finaltrainset[test_index]\n",
    "    \n",
    "    print(\"Fold\", fold, X_training.shape, X_valid.shape)\n",
    "    \n",
    "    print('Fitting XGBoost for Fold {}'.format(fold))\n",
    "    dtrain = xgb.DMatrix(X_training, y_training)\n",
    "    dtest = xgb.DMatrix(X_valid)\n",
    "    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "    \n",
    "    print('Fitting stacked pipeline for Fold {}'.format(fold))\n",
    "    stacked_pipeline.fit(final_train, y_training)\n",
    "    \n",
    "    print(r2_score(y_valid,stacked_pipeline.predict(final_valid)*0.2855 + model.predict(dtest)*0.7145))\n",
    "\n",
    "    \n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
