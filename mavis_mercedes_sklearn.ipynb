{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed all data y >160\n",
      "There were 5 outliers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from subprocess import check_output\n",
    "from sklearn import metrics, model_selection\n",
    "# /Users/mli3/ml/Kaggle/train.csv\n",
    "\n",
    "def load_data(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# load the data\n",
    "csv_path = '/Users/mli3/ml/Kaggle/'\n",
    "train = load_data(csv_path + \"train.csv\")\n",
    "test = load_data(csv_path + \"test.csv\")\n",
    "\n",
    "\n",
    "train.describe()\n",
    "\n",
    "# remove columns where the value is the same for all rows (no new information)\n",
    "nunique = train.apply(pd.Series.nunique)\n",
    "col_same_train = list(nunique[nunique==1].index)\n",
    "train2 = train.drop(col_same_train,1)\n",
    "\n",
    "# apply the same to the test set\n",
    "test2 = test.drop(col_same_train, 1)\n",
    "# remove columns where the column is same as another column\n",
    "\n",
    "# hist of Y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "y_train = train2[[\"y\"]]\n",
    "y_train.hist(bins=200, figsize=(20,15))\n",
    "#plt.show()\n",
    "\n",
    "# remove row without the maximum y (looks like outliet)\n",
    "outlier_limit = 160\n",
    "train3 = train2.loc[train2['y']<=outlier_limit]\n",
    "\n",
    "\n",
    "print(\"We removed all data y >\" + str(outlier_limit))\n",
    "print(\"There were \" + str(sum(train2['y']>outlier_limit)) + \" outliers\")\n",
    "\n",
    "c = train3.corr()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind='quicksort', ascending=False)\n",
    "so['y']\n",
    "\n",
    "# remove columns that are identical and also those that are complete opposite\n",
    "col_same = s[((s==1) | (s==-1)) & (s.index.get_level_values(0) != s.index.get_level_values(1))]\n",
    "#keep the smaller of the columns\n",
    "# if x is smaller than the other columns, throw other columns out, else ignore\n",
    "dup_columns = set()\n",
    "for x,y in col_same.index:\n",
    "    if int(x[1:]) < int(y[1:]):\n",
    "        dup_columns.add(y)\n",
    "\n",
    "# there are 45+8 dup_columns\n",
    "train4 = train3.drop(list(dup_columns), 1)\n",
    "test4 = test2.drop(list(dup_columns), 1)\n",
    "# impute missing values\n",
    "\n",
    "# # this part might be unecessary\n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "# df_dedup_num = df_dedup.drop([\"ID\", \"y\", \"X0\",  \"X1\",  \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"], axis=1)\n",
    "# imputer.fit(df_dedup_num)\n",
    "# imputer.statistics_\n",
    "# df_dedup_num.median().values\n",
    "# X = imputer.transform(df_dedup_num)\n",
    "# df_dedup_num_tr = pd.DataFrame(X, columns=df_dedup_num.columns)\n",
    "\n",
    "# 1 hot encoding\n",
    "\n",
    "\n",
    "train_num = train4.drop([\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"], 1) # kept y column\n",
    "train_cat = train4.loc[:, [\"ID\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]]\n",
    "\n",
    "\n",
    "\n",
    "test_num = test4.drop([\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"], 1)\n",
    "test_cat = test4.loc[:, [\"ID\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]]\n",
    "#from sklearn.preprocessing import LabelBinarizer\n",
    "#encoder = LabelBinarizer()\n",
    "#data_dedup_cat_1hot = encoder.fit_transform(data_dedup_cat)\n",
    "#data_dedup_cat_1hot\n",
    "\n",
    "\n",
    "train_one_hot_cat = pd.get_dummies(train_cat, columns=[\"X0\", \"X1\",  \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"])\n",
    "train_one_hot = train_one_hot_cat.merge(train_num, on='ID')\n",
    "\n",
    "test_one_hot_cat = pd.get_dummies(test_cat, columns=[\"X0\", \"X1\",  \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"])\n",
    "test_one_hot = test_one_hot_cat.merge(test_num, on='ID')\n",
    "\n",
    "X = train_one_hot.drop(['ID', 'y'], axis=1)\n",
    "y = train_one_hot['y']\n",
    "\n",
    "\n",
    "feature_labels = X.columns\n",
    "forest = RandomForestRegressor(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "forest.fit(X, y)    # Fits the Random Forest Regressor to the entire data set.\n",
    "importances = forest.feature_importances_  # Sets importances equal to the feature importances of the model\n",
    "indices = np.argsort(importances)[::-1]\n",
    "order_features = []\n",
    "order_importances = []\n",
    "for f in range(X.shape[1]):\n",
    "    #print(\"%2d) %-*s %f\" % (f+1, 30, feature_labels[indices[f]], importances[indices[f]]))\n",
    "    order_features.append(feature_labels[f])\n",
    "    order_importances.append(importances[indices[f]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done using 350 columns\n",
      "Linear Regression: \n",
      "    Scores: [ 0.32353491  0.5955188   0.45857671  0.57692441  0.62334115  0.5233004\n",
      "  0.4579369   0.53408989  0.36919079 -0.05846554]\n",
      "    Mean: 0.440394842084\n",
      "    Std dev: 0.189750218777\n",
      "Tree Regression: \n",
      "    Scores: [-0.44190576  0.23518675 -0.84575501  0.36318079  0.17690694  0.12878081\n",
      "  0.16517882  0.28162083  0.43695896  0.38972555]\n",
      "    Mean: 0.0889878684166\n",
      "    Std dev: 0.389345727904\n",
      "Forest:\n",
      "    Scores: [ 0.46544611  0.50011556  0.22269508  0.54746536  0.61043793  0.48753119\n",
      "  0.45301922  0.57125111  0.59063383  0.59079683]\n",
      "    Mean: 0.503939221135\n",
      "    Std dev: 0.107835181427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ncol=350\n",
    "print(\"Training is done using \" + str(ncol) + \" columns\")\n",
    "train = train_one_hot.drop(order_features[ncol:], axis=1) # Modify train to only take in the top 100 features and the target column y\n",
    "test = test_one_hot.loc[:,list(train.columns)]\n",
    "test = test.drop([\"y\"], axis=1).fillna(0)\n",
    "\n",
    "y_train = train['y']\n",
    "train = train.drop([\"y\"], axis=1)\n",
    "\n",
    "\n",
    "#print(\"Using PCA\")\n",
    "#from sklearn import decomposition\n",
    "#pca = decomposition.PCA()\n",
    "#pca.fit(X)\n",
    "\n",
    "#train = pca.fit_transform(train)[:,:ncol]\n",
    "#test = pca.transform(test)[:,:ncol]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def display_scores(scores):\n",
    "    print(\"    Scores:\", scores)\n",
    "    print(\"    Mean:\", scores.mean())\n",
    "    print(\"    Std dev:\", scores.std())\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train, y_train)\n",
    "lin_reg_scores = cross_val_score(lin_reg, train, y_train, \n",
    "    scoring=\"r2\", cv=10)\n",
    "\n",
    "print(\"Linear Regression: \")\n",
    "display_scores(lin_reg_scores)\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#housing_predictions = lin_reg.predict(housing_prepared)\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(train, y_train)\n",
    "tree_reg_scores = cross_val_score(tree_reg, train, y_train,  \n",
    "    scoring=\"r2\", cv=10)\n",
    "\n",
    "print (\"Tree Regression: \" )\n",
    "display_scores(tree_reg_scores)\n",
    "\n",
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(train, y_train)\n",
    "forest_scores = cross_val_score(forest_reg, train, y_train, \n",
    "    scoring=\"r2\", cv=10)\n",
    "\n",
    "print (\"Forest:\" )\n",
    "display_scores(forest_scores)\n",
    "\n",
    "\n",
    "# Support Vector Machine\n",
    "#from sklearn.svm import SVR\n",
    "#svm_linear = SVR(kernel=\"linear\", C=1.0, gamma='auto')\n",
    "#svm_linear.fit(train, y_train)\n",
    "#svm_linear_scores = cross_val_score(svm_linear, train, y_train,\n",
    "#    scoring=\"r2\", cv=10)\n",
    "\n",
    "#print (\"SVM Linear:\" )\n",
    "#display_scores(svm_linear_scores)\n",
    "\n",
    "# rbf\n",
    "#svm_rbf = SVR(kernel=\"rbf\", C=1.0, gamma='auto')\n",
    "#svm_rbf.fit(train, y_train)\n",
    "#svm_rbf_scores = cross_val_score(svm_rbf, train, y_train, \n",
    "#    scoring=\"r2\", cv=10)\n",
    "\n",
    "#print (\"SVM rbf\" )\n",
    "#display_scores(svm_rbf_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
